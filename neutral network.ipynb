{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y = 2x + 1 --> x and y 之间存在关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "没有免费午餐(No free lunch theorem)：这是在机器学习中被证明的定理。就是说，对于某个问题高效的优化算法一定在另一个问题上存在缺陷，不可能对于所有问题都有效。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 为什么神经网络能够识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络中的层主要作用有 *升维/降维 、放大/缩小、旋转、平移、弯曲*\n",
    "每一层的功能用数学解释就是：用线性变换跟随着非线性变换，将输入空间投向另一个空间。\n",
    "每一层的物理理解就是：通过现有的不同物质的组合形成新物质。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 这种模式如何完成识别任务？\n",
    "超平面（比当前空间低一维的子空间，加入当前空间是直线的话，超平面就是点）\n",
    "例如在一条直线上面分离 正数，负数或者零的时候，可以采用两个超平面进行分离。\n",
    "但是分离奇数、偶数却不能采用超平面的方法，此时可以用 x%2 的转变来进行划分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络就是采用上面所说的五种方法来将事情进行分类。\n",
    "\n",
    "*所谓监督学习就是给予神经网络网络大量的训练例子，让网络从训练例子中学会如何变换空间。每一层的权重 W 就控制着如何变换空间，我们最终需要的也就是训练好的神经网络的所有层的权重矩阵。*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性可分视角：神经网络的学习就是学习如何利用矩阵的线性变换加激活函数的非线性变换，将原始输入空间投向线性可分/稀疏的空间去分类/回归。\n",
    "增加节点数：增加维度，即增加线性转换能力。\n",
    "增加层数：增加激活函数的次数，即增加非线性转换次数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "物质组成视角：神经网络的学习过程就是学习物质组成方式的过程。\n",
    "增加节点数：增加同一层物质的种类，比如118个元素的原子层就有118个节点。\n",
    "增加层数：增加更多层级，比如分子层，原子层，器官层，并通过判断更抽象的概念来识别物体。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么就可以通过比较当前网络的预测值和我们真正想要的目标值，再根据两者的差异情况来更新每一层的权重矩阵（比如，如果网络的预测值高了，就调整权重让它预测低一些。不断调整，直到能够预测出目标值）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此就需要先定义“如何比较预测值和目标值的差异”，这便是损失函数或目标函数（loss function or objective function），用于衡量预测值和目标值的差异的方程。loss function的输出值（loss）越高表示差异性越大。那神经网络的训练就变成了尽可能的缩小loss的过程。\n",
    "+\n",
    "所用的方法是梯度下降（Gradient descent）：通过使loss值向当前点对应梯度的反方向不断移动，来降低loss。一次移动多少是由学习速率（learning rate）来控制的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "试图解决“卡在局部极小值”问题的方法分两大类：\n",
    "\n",
    "调节步伐：调节学习速率，使每一次的更新“步伐”不同。常用方法有：\n",
    "\n",
    "随机梯度下降（Stochastic Gradient Descent (SGD)：每次只更新一个样本所计算的梯度\n",
    "\n",
    "小批量梯度下降（Mini-batch gradient descent）：每次更新若干样本所计算的梯度的平均值\n",
    "\n",
    "动量（Momentum）：不仅仅考虑当前样本所计算的梯度；Nesterov动量（Nesterov Momentum）：Momentum的改进\n",
    "\n",
    "Adagrad、RMSProp、Adadelta、Adam：这些方法都是训练过程中依照规则降低学习速率，部分也综合动量\n",
    "\n",
    "优化起点：合理初始化权重（weights initialization）、预训练网络（pre-train），使网络获得一个较好的“起始点”，如最右侧的起始点就比最左侧的起始点要好。常用方法有：高斯分布初始权重（Gaussian distribution）、均匀分布初始权重（Uniform distribution）、Glorot 初始权重、He初始权、稀疏矩阵初始权重（sparse matrix）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习所处理的数据都是高维数据，该如何快速计算梯度、而不是以年来计算。 其次如何更新隐藏层的权重？ 解决方法是：计算图：反向传播算法  http://colah.github.io/posts/2015-08-Backprop/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLu 激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.占位符（placeholder）\n",
    "\n",
    "用来占住所用数据所需的位置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.构建图表 （graph）\n",
    "\n",
    "构建图表有三个阶段的模式函数操作：inference（）、loss（）、training（）\n",
    "\n",
    "其中，inference（） 是为了尽可能好地构建图表，满足促使神经网络向前反馈并且做出预测的要求。\n",
    "\n",
    "loss（）是往 inference 图表中添加生成损失（loss）所需要的操作（ops）。\n",
    "\n",
    "training（） 是往损失图表中添加计算并应用梯度（gradients）所需要的操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Inference（推理）\n",
    "\n",
    "inference（）函数会尽可能地构建图表，做到返回包含了预测结果（output prediction）的 Tensor。\n",
    "\n",
    "它接受图像占位符为输入，在此基础上借助ReLu(Rectified Linear Units)激活函数，构建一对完全连接层（layers），以及一个有着十个节点（node）、指明了输出logtis模型的线性层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "2维输入 --》2维隐藏 --》1维输出 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "D_input = 2\n",
    "D_label  = 1\n",
    "D_hidden = 2\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "容器？？？？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, D_input], name='x')\n",
    "y = tf.placeholder(tf.float32, [None, D_label], name='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "W_h1 = tf.Variable(tf.truncated_normal([D_input, D_hidden], stddev=0.1), name=\"W_h\")\n",
    "b_h1 = tf.Variable(tf.constant(0.1, shape=[D_hidden]), name=\"b_h\")\n",
    "pre_act_h1 = tf.matmul(x, W_h1) + b_h1\n",
    "act_h1 = tf.nn.relu(pre_act_h1, name='act_h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "W_o = tf.Variable(tf.truncated_normal([D_hidden, D_label], stddev=0.1), name=\"W_o\")\n",
    "b_o = tf.Variable(tf.constant(0.1, shape=[D_label]), name=\"b_o\")\n",
    "pre_act_o = tf.matmul(act_h1, W_o) + b_o\n",
    "y = tf.nn.relu(pre_act_o, name='act_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
